# Thomas Hoffmann, EMBL Heidelberg, structures-it@embl.de, 2023/07
easyblock = 'PythonBundle'

name = 'flash-attn'
version = '2.0.3'
versionsuffix = '-CUDA-%(cudaver)s'

homepage = 'https://github.com/Dao-AILab/flash-attention'
description = """This repository provides the official implementation of FlashAttention and      
FlashAttention-2 from the following papers.

FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness     
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher RÃ©
Paper: https://arxiv.org/abs/2205.14135
IEEE Spectrum article about our submission to the MLPerf 2.0 benchmark using    
FlashAttention.
"""

toolchain = {'name': 'foss', 'version': '2022a'}

dependencies = [
    ('Python', '3.10.4'),
    ('CUDA', '11.7.0', '', SYSTEM),
    ('PyTorch', '1.12.0', versionsuffix),
    ('einops', '0.4.1'),
]

use_pip = True
sanity_pip_check = True

exts_list = [
    ('flash_attn', version, {
        'checksums': ['f198a8835a4a29c41da81b8ee32b08007f90dad883405fbdb008358457a87546'],
    }),
]

sanity_check_paths = {
    'files': [],
    'dirs': ["lib/python%(pyshortver)s/site-packages/flash_attn"]
}

moduleclass = 'lib'
