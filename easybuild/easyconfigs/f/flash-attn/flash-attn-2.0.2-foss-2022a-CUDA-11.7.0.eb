# Thomas Hoffmann, EMBL Heidelberg, structures-it@embl.de, 2023/07
easyblock = 'PythonBundle'

name = 'flash-attn'
version = '2.0.2'
versionsuffix = '-CUDA-%(cudaver)s'

homepage = 'https://github.com/Dao-AILab/flash-attention'
description = """This repository provides the official implementation of FlashAttention and      
FlashAttention-2 from the following papers.

FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness     
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher RÃ©
Paper: https://arxiv.org/abs/2205.14135
IEEE Spectrum article about our submission to the MLPerf 2.0 benchmark using    
FlashAttention.
"""

toolchain = {'name': 'foss', 'version': '2022a'}

dependencies = [
    ('Python', '3.10.4'),
    ('CUDA', '11.7.0', '', SYSTEM),
    ('PyTorch', '1.12.0', versionsuffix),
    ('einops', '0.4.1'),
]

use_pip = True
exts_download_dep_fail = True
exts_list = [
    ('flash_attn', version, {
        'checksums': ['3cbc5b17c551987f992901d401da5da2a37e30aa1bb78aa76e2458bbca66377a'],
    }),
]

sanity_check_paths = {
    'files': [],
    'dirs': ["lib/python%(pyshortver)s/site-packages/flash_attn"]
}

moduleclass = 'lib'
